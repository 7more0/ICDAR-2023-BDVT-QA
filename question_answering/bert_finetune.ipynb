{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\jeomin\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\squad\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Sat Mar 25 14:33:01 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset squad (C:/Users/jeomin/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 45.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# load squad\n",
    "from datasets import load_dataset, load_metric\n",
    "squad_v2 = False\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/jeomin/.cache/huggingface/datasets/json/default-6004272bb7647219/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/jeomin/.cache/huggingface/datasets/json/default-6004272bb7647219/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# load our dataset\n",
    "# https://huggingface.co/docs/datasets/loading#from-local-files\n",
    "from datasets import load_dataset\n",
    "dataset_file = 'data/our_training_dataset.json'\n",
    "datasets = load_dataset(\"json\", data_files=dataset_file)\n",
    "datasets = datasets['train']\n",
    "datasets = datasets.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers'],\n",
       "        num_rows: 948\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers'],\n",
       "        num_rows: 106\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data preprocess tokenize\n",
    "1/Some examples in a dataset may have a very long context that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the context by setting truncation=\"only_second\".\\\n",
    "2/Next, map the start and end positions of the answer to the original context by setting return_offset_mapping=True.\\\n",
    "3/With the mapping in hand, now you can find the start and end tokens of the answer. Use the sequence_ids method to find which part of the offset corresponds to the question and which corresponds to the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForQuestionAnswering, Trainer, TrainingArguments, DefaultDataCollator\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# hyperparam\n",
    "model_checkpoint = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384\n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 64\n",
    "\n",
    "# load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "assert isinstance(tokenizer, PreTrainedTokenizerFast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://huggingface.co/docs/transformers/main/en/training#train-in-native-pytorch\n",
    "https://huggingface.co/docs/transformers/main/en/tasks/question_answering\n",
    "\"\"\"\n",
    "def preprocess_function(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=doc_stride,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        \n",
    "        # Start token index of the current span in the text.\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  98%|█████████▊| 86000/87599 [00:58<00:00, 3168.75 examples/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, \n",
    "we just use the map method of our dataset object we created earlier. \n",
    "This will apply the function on all the elements of all the splits in dataset, so our training,\n",
    "validation and testing data will be preprocessed in one single command. \n",
    "Since our preprocessing changes the number of samples, we need to remove the old columns when applying it.\n",
    "\"\"\"\n",
    "tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.Define your training hyperparameters in TrainingArguments. \n",
    "The only required parameter is output_dir which specifies where to save your model. \n",
    "You'll push this model to the Hub by setting push_to_hub=True \n",
    "(you need to be signed in to Hugging Face to upload your model).\n",
    "\"\"\"\n",
    "batch_size = 8\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"comp_qa_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BertForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "\"\"\"\n",
    "2.Pass the training arguments to Trainer along with the model, dataset, tokenizer, and data collator.\n",
    "\"\"\"\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))\n",
    "dataloader = DefaultDataCollator()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.fine tune the model.\n",
    "\"\"\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model if you need.\n",
    "trainer.save_model(\"test-squad-trained\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess\n",
    "https://huggingface.co/docs/transformers/main/en/preprocessing\n",
    "https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb\n",
    "\n",
    "fine_tune\n",
    "https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb\n",
    "https://huggingface.co/docs/transformers/main/en/training#train-in-native-pytorch\n",
    "\n",
    "qa\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "https://huggingface.co/docs/transformers/main/en/tasks/question_answering\n",
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchwithgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
