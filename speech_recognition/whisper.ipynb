{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21372,"status":"ok","timestamp":1678279321467,"user":{"displayName":"zhaomin li","userId":"07249756473721386513"},"user_tz":-480},"id":"dr1exXIP3gk3","outputId":"4697c2f2-1d8f-485a-92ec-c0aecf933e44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# @title Load Cloud Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1064,"status":"ok","timestamp":1677938798667,"user":{"displayName":"zhaomin li","userId":"07249756473721386513"},"user_tz":-480},"id":"fLIiQs2g09aP","outputId":"a49bcd18-a0d9-41c8-c4ff-03cc00c7134c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Mar  4 14:06:37 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   66C    P0    31W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# @title show GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6253,"status":"ok","timestamp":1677938809772,"user":{"displayName":"zhaomin li","userId":"07249756473721386513"},"user_tz":-480},"id":"BAAQIqLl32Pk","outputId":"057f876e-c7b9-4569-fa84-686d43a379a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-jlujut2z\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-jlujut2z\n","  Resolved https://github.com/openai/whisper.git to commit 3e1780fd37686666f568be9c99f5b5e3e4f2eb92\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.22.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.1.0)\n","Requirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.26.1)\n","Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (0.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (0.13.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2.25.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (0.12.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2.10)\n"]}],"source":["# @title Setting Up\n","# install packages\n","!pip install git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KooCgl3ET0DZ"},"outputs":[],"source":["# @title Set Path\n","import os\n","# add folders\n","DATAPATH = '/content/drive/MyDrive/dataset'\n","RESPATH = '/content/drive/MyDrive/result'\n","check_dataset_folder = os.path.exists(DATAPATH)\n","check_download_folder = os.path.exists(RESPATH)\n","if not check_dataset_folder:\n","  os.mkdir(DATAPATH)\n","if not check_download_folder:\n","  os.mkdir(RESPATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56681,"status":"ok","timestamp":1677938881349,"user":{"displayName":"zhaomin li","userId":"07249756473721386513"},"user_tz":-480},"id":"I4BvucSq61RH","outputId":"9d592f55-0bbc-4d52-9885-0f46efd15f55"},"outputs":[],"source":["# @title Transcription\n","import whisper\n","file_name = \"vid_6371842732458037.mp4\" #@param{type:\"string\"}\n","model_type = \"medium\" #@param[\"base\",\"small\",\"medium\",\"large\",\"large-v2\"]\n","model = whisper.load_model(model_type)\n","\n","# load audio and pad/trim it to fit 30 seconds\n","audio = whisper.load_audio(DATAPATH + f\"/{file_name}\")\n","audio = whisper.pad_or_trim(audio)\n","\n","# make log-Mel spectrogram and move to the same device as the model\n","mel = whisper.log_mel_spectrogram(audio).to(model.device)\n","\n","# detect the spoken language\n","_, probs = model.detect_language(mel)\n","print(f\"Detected language: {max(probs, key=probs.get)}\")\n","\n","# decode the audio\n","options = whisper.DecodingOptions()\n","result = whisper.decode(model, mel, options)\n","\n","# print the recognized text\n","print(result.text)\n","\n","# Write into a text file\n","name = RESPATH + \"/\" + file_name.split('.')[0]\n","with open(f\"{name}.txt\", \"w\") as f:\n","  # f.write(f\"▼ Transcription of {file_name}\\n\")\n","  f.write(result.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQYvHKgZNtN6"},"outputs":[],"source":["#@title Download the transcription file\n","from google.colab import files\n","!zip -r download.zip download\n","files.download(\"result.zip\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████| 1.42G/1.42G [08:27<00:00, 3.01MiB/s]\n"]},{"ename":"MemoryError","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32md:\\staticData\\Data\\project\\videos2text\\whisper.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/staticData/Data/project/videos2text/whisper.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# file_name = \"vid_6371842732458037.mp4\" #@param{type:\"string\"}\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/staticData/Data/project/videos2text/whisper.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmedium\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#@param[\"base\",\"small\",\"medium\",\"large\",\"large-v2\"]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/staticData/Data/project/videos2text/whisper.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39;49mload_model(model_type)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/staticData/Data/project/videos2text/whisper.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_name \u001b[39min\u001b[39;00m tqdm(file_name_list):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/staticData/Data/project/videos2text/whisper.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m    \u001b[39mif\u001b[39;00m file_name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.mp4\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/staticData/Data/project/videos2text/whisper.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m# load audio and pad/trim it to fit 30 seconds\u001b[39;00m\n","File \u001b[1;32md:\\SoftWare\\AnaConda\\install\\envs\\torchwithgpu\\lib\\site-packages\\whisper\\__init__.py:131\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[0;32m    128\u001b[0m     download_root \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mXDG_CACHE_HOME\u001b[39m\u001b[39m\"\u001b[39m, default), \u001b[39m\"\u001b[39m\u001b[39mwhisper\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _MODELS:\n\u001b[1;32m--> 131\u001b[0m     checkpoint_file \u001b[39m=\u001b[39m _download(_MODELS[name], download_root, in_memory)\n\u001b[0;32m    132\u001b[0m     alignment_heads \u001b[39m=\u001b[39m _ALIGNMENT_HEADS[name]\n\u001b[0;32m    133\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(name):\n","File \u001b[1;32md:\\SoftWare\\AnaConda\\install\\envs\\torchwithgpu\\lib\\site-packages\\whisper\\__init__.py:83\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(url, root, in_memory)\u001b[0m\n\u001b[0;32m     80\u001b[0m             output\u001b[39m.\u001b[39mwrite(buffer)\n\u001b[0;32m     81\u001b[0m             loop\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(buffer))\n\u001b[1;32m---> 83\u001b[0m model_bytes \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(download_target, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m hashlib\u001b[39m.\u001b[39msha256(model_bytes)\u001b[39m.\u001b[39mhexdigest() \u001b[39m!=\u001b[39m expected_sha256:\n\u001b[0;32m     85\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m     )\n","\u001b[1;31mMemoryError\u001b[0m: "]}],"source":["import whisper\n","from os import listdir\n","from tqdm import tqdm\n","file_path = \"dataset/train/train_videos_part1\"\n","RESPATH = \"dataset/train/videos_result\"\n","\n","file_name_list=listdir(path=file_path)\n","# file_name = \"vid_6371842732458037.mp4\" #@param{type:\"string\"}\n","\n","model_type = \"medium\" #@param[\"base\",\"small\",\"medium\",\"large\",\"large-v2\"]\n","model = whisper.load_model(model_type)\n","\n","for file_name in tqdm(file_name_list):\n","   if file_name.endswith('.mp4'):\n","        # load audio and pad/trim it to fit 30 seconds\n","        audio = whisper.load_audio(file_path + f\"/{file_name}\")\n","        audio = whisper.pad_or_trim(audio)\n","\n","        # make log-Mel spectrogram and move to the same device as the model\n","        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n","\n","        # detect the spoken language\n","        _, probs = model.detect_language(mel)\n","        # print(f\"Detected language: {max(probs, key=probs.get)}\")\n","\n","        # decode the audio\n","        options = whisper.DecodingOptions()\n","        result = whisper.decode(model, mel, options)\n","\n","        # print the recognized text\n","        print(result.text)\n","\n","        # Write into a text file\n","        name = RESPATH + \"/\" + file_name.split('.')[0]\n","        with open(f\"{name}.txt\", \"w\") as f:\n","        # f.write(f\"▼ Transcription of {file_name}\\n\")\n","            f.write(result.text)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset\train\train_videos_part1/vid_6371842732458037.mp4\n","... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...\n"]}],"source":["from os import listdir\n","import whisper\n","# PATH=\"dataset/train/train_videos_part\"\n","# filelist = listdir(path=PATH+\"1\")\n","file_path = \"dataset\\train\\train_videos_part1\"\n","RESPATH = \"dataset/train/videos_result\"\n","# file_name_list = []\n","# names = [\"1\",\"2\",\"3\"]\n","# for name in names:\n","# file_name_list=listdir(path=file_path)\n","# count = 0\n","# for file_name in tqdm(file_name_list):\n","#    if file_name.endswith('.mp4'):\n","#       print(count)\n","#       count += 1\n","file_name = \"vid_6371842732458037.mp4\"\n","name = RESPATH + \"/\" + file_name.split('.')[0]\n","print(file_path + f\"/{file_name}\")\n","\n","model_type = \"base\" #@param[\"base\",\"small\",\"medium\",\"large\",\"large-v2\"]\n","model = whisper.load_model(model_type)\n","# audio = whisper.load_audio(file_path + f\"\\{file_name}\")\n","audio = whisper.load_audio('dataset/train/train_videos_part1/vid_6371842732441601.mp4')\n","audio = whisper.pad_or_trim(audio)\n","\n","# make log-Mel spectrogram and move to the same device as the model\n","mel = whisper.log_mel_spectrogram(audio).to(model.device)\n","\n","# detect the spoken language\n","_, probs = model.detect_language(mel)\n","# print(f\"Detected language: {max(probs, key=probs.get)}\")\n","\n","# decode the audio\n","options = whisper.DecodingOptions()\n","result = whisper.decode(model, mel, options)\n","\n","# print the recognized text\n","print(result.text)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNRahZqzS8zLlwUqM2NB2+V","mount_file_id":"1yCRqzabdpykOqMCM8I5fd-sF9Su1vppX","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
